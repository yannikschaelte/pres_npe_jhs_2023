<!doctype html>
<html>


<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <title>JHS First Meeting</title>

  <meta name="description" content="Presentation for the JHS Add-On 8th Cohort First Meeting, Hamburg 2023-02-11">
  <meta name="author" content="Yannik Schaelte">

  <link rel="stylesheet" href="reveal.js/dist/reset.css">
  <link rel="stylesheet" href="reveal.js/dist/reveal.css">
  <link rel="stylesheet" href="ywhite.css" id="theme">

  <!-- Theme used for syntax highlighted code -->
  <link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">

  <link rel="stylesheet" href="ystyle.css">
</head>


<!--
Helmholtz colors:
Purple #69005F
Red #FF506E
-->

<!--
Bonn colors:
Blue #07529a
Yellow #eab90c
-->


<body>
  <div class="reveal">
    <div class="slides" id="id_slides">

      <section data-transition="slide-in fade-out">
        <img src="img/front.svg" alt="Front page" class="r-stretch"/>
      </section>

      <!--<section>
        <a href="https://yannikschaelte.github.io/pres_npe_jhs_2023">yannikschaelte.github.io/pres_npe_hjs_2023</a>
      </section>-->

      <section>
        <div class="r-stack" id="id_sciml_div" style="padding: 0px; margin: 0px;">
            <script>
                var node = document.getElementById("id_sciml_div");
                node.innerHTML += "<img src='img/sciml1.svg'/>"
                var i;
                for (i = 2; i <= 5; i++) {
                    node.innerHTML += "<img src='img/sciml" + i + ".svg' class='fragment'/>"
                }  
            </script>
        </div>
      </section>

      <section>
        <h2>mixed-effects modeling</h2>
        <img src="img/mixed_effects.svg"/>
        <div style="text-align: left;">
          <table style="border: none;" class="fragment">
            <tr style="border: none;"><td style="border: none;">dynamical model:</td><td style="border: none;">$\dot x = f(x,\theta)$</td></tr>
            <tr style="border: none;"><td style="border: none;">observables:</td><td style="border: none;">$y = h(x, \theta) + \varepsilon$</td></tr>
            <tr style="border: none;"><td style="border: none;">parameters:</td><td style="border: none;">$\theta = A\alpha + B\beta,\quad\beta\sim\mathcal{N}(0,\Sigma)$</td></tr>
          </table>
          
        </div>
      </section>

      <section>
        <h2>the problem</h2>
        <ul style="text-align: left;">
          <li>
            <b>estimate parameters:</b> maximize over $\alpha$ and $\Sigma$ the likelihood of data $y_\text{obs}$, marginalized over random effects $\beta$,
            $$\pi(y_\text{obs}|\alpha,\Sigma) = \prod_i\int\pi(y_i|\theta)\pi(\theta|\alpha,\Sigma)d\theta$$
          </li>
          <li class="fragment" data-fragment-index="1"><b style="color: #5c2874;">problem: evaluating these (high-dim) integrals is challenging</b>,<br/>
            especially with many individuals<br/><br/></li>
        </ul>
        <img class="fragment" data-fragment-index="1" src="img/compute_time.svg" class="r-stretch"/>
      </section>
      
      <section>
        <h2>amortized inference via invertible neural networks</h2>
        you have to solve many similar problems? amortize the solution!
        <img src="img/bayesflow.svg" class="r-stretch"/>
        <div style="text-align:right;"><small>similar to: Radev et al., 2021</small></div>
      </section>

      <section>
        <ul style="text-align: left;">
          <li>
            parameter estimation requires repeatedly evaluating an integral
            $$\pi(y_\text{obs}|\alpha,\Sigma) = \prod_i\int\pi(y_i|\theta)\pi(\theta|\alpha,\Sigma)d\theta$$
          </li>
        </ul>
        <br/>
        <h2 class="fragment">our method</h2>
        <ul style="text-align: left;">
          <li class="fragment" style="text-align: left;">
            <b>idea:</b> rewrite in terms of an <b>individual-specific posterior:</b>
            $$\begin{split}\pi(y_\text{obs}|\alpha,\Sigma) &= \prod_i\pi(y_i)\int\pi(\theta|y_i)\frac{\pi(\theta|\alpha,\Sigma)}{\pi(\theta)}d\theta \\&= \prod_i\pi(y_i)\mathbb{E}_{\theta\sim\pi(\theta|y_i)}\left[\frac{\pi(\theta|\alpha,\Sigma)}{\pi(\theta)}\right]\end{split}$$
          </li>
          <li class="fragment">... and approximate the posterior using a <b style="color: #5c2874;">neural density estimator</b> trained on synthetic data!</li>
        </ul>
      </section>

      <section>
        <h2>results</h2>
        <ul style="list-style-type: 'âœ“ ';">
          <li class="fragment" data-fragment-index="1">NN <b>fits the posterior</b> well</li>
          <li class="fragment" data-fragment-index="2">after training once ($1-2h$), the optimization is <b>very fast</b> ($s-min$)</li>
          <li class="fragment" data-fragment-index="3">even <b>uncertainty analysis</b> easily possible</li>
          <li class="fragment" data-fragment-index="4">can handle multiple <b>experiment conditions</b></li>
        </ul>
        <div class="r-stack">
          <img class="fragment current-visible" data-fragment-index="1" src="img/ame_results_fit.svg" height="250px"/>
          <img class="fragment current-visible" data-fragment-index="2" src="img/ame_results_speedup.svg" height="250px"/>
          <img class="fragment" data-fragment-index="3" src="img/ame_results_uncertainty.png" height="250px"/>
        </div>
      </section>

      <section data-background-image="img/missing_piece_puzzle.jpg" data-background-transition="zoom"
        data-background-opacity="0.1">
        <img src="img_npe/missing_datas.svg" class="r-stretch fragment" />
        <br /><br />
        <h2 style="text-align:left;">How to handle missing data in amortized inference?</h2>
      </section>

      <section>
        <h2>Problem: BayesFlow cannot interpret the data</h2>
        <img src="img_npe/deletion.png" class="r-stretch" />
      </section>

      <section>
        <h2>Encode missing data</h2>
        <img src="img_npe/concept_bayesflow_missingdata2.png" class="r-stretch" />
      </section>

      <section>
      <section>
        All approaches perform well on simple test problem<br/>
        <img src="img_npe/icon_cr.svg" height="60px"/>
        <img src="img_npe/CR3.png" class="r-stretch" />
      </section>

      <section>
        Binary indicator augmentation more<br />robust for ambiguous fill-in values
        <img src="img_npe/Figure3.svg" class="r-stretch" />
      </section>

      <section>
        Time labels encoding performs not robustly in case of oscillatory data<br/>
        <img src="img_npe/icon_sin.svg" height="60px"/><br/>
        <img src="img_npe/Osc_augment_time_quantile.png" class="r-stretch" />
      </section>

      <section>
        <img src="img_npe/Osc_convergence_log.svg" />
      </section>

      <section>
        Variable dataset size as a special case of missing data<br/>
        <img src="img_npe/Variable_length_loss.svg" width="48%" />
        <img src="img_npe/Variable_length_rl.svg" width="48%" />
        Augment by 0/1 improves performance due to better cost function approximation with individual-specific missingness
      </section>

      <section>
        Scales to more complex inference problems<br/>
        <img src="img_npe/icon_sir.svg" height="60px"/><br/>
        <img src="img_npe/SIR.png" class="r-stretch" />
      </section>

      <section>
        Able to unravel parameter-dependent missingness<br/>
        <img src="img_npe/CR11_missingness_par_augment01.png" class="r-stretch" />
      </section>

      <section>
        <h2>Can we just impute missing values?</h2>
        <div class="r-stack">
          <img src="img_npe/impute1.svg" width="80%" />
          <img src="img_npe/impute2.svg" width="80%" class="fragment" />
        </div>
      </section>

      <section>
        Inappropriate imputation can lead to biased results
        <img src="img_npe/linear_interpolation.png" class="r-stretch" />
      </section>

      <section>
        <h3>There are no free data</h3>
        <div style="text-align: left; font-size: 10pt;">
          Imputation means that instead of working with available data $x$, we try to reconstruct the full data $\bar
          x$,
          and estimate parameter probabilities $\pi(\theta|\bar x)$ instead of $\pi(\theta|x)$.
          However, the true full data are unknown, therefore we need to take uncertainty in $\bar x$ into account,
          considering a full distribution of values $\pi(\bar x|x)$.

          <p />
          We must either make up a distribution (introducing a bias), or use a faithful approximation
          $p(\bar x|x) = \pi(\bar x|x)$ where $\pi(\bar x|x)\pi(x) = \pi(\bar x, x)$.

          <p />
          However, if we integrate out over all possible realizations of full data, we obtain
          $\int \pi(\theta|\bar x)\pi(\bar x|x)d\bar x = \pi(\theta|x)$ (or similarly $\pi(\theta|x,\tau)$).
        </div>
        TLDR: When doing uncertainty quantification properly, we just recover the same posterior.
      </section>
      </section>

      <section data-background-image="img/questions.jpg" data-background-opacity="0.3">
        <h1 style="color: #5c2874ff;">Thanks! Questions?</h1>
      </section>

       <section>
          <h2>BayesFlow</h2>
          <img src="img_npe/bayesflow_concept.png" />
          <div style="text-align: right;"><small>from: Radev et al, IEEE Transactions on Neural Networks 2020</small>
          </div>
        </section>

        <section>
          <p>
          <h3>The problem</h3>
          <ul>
            <li>Classical simulation-based inference is case-based + slow + $\varepsilon$-approximate</li>
            <li>What if we want to fit the same model to multiple datasets?</li>
          </ul>
          </p>
          <p>
          <h3>The idea</h3>
          <ul>
            <li>Learn a global estimator for the probabilistic mapping $y\mapsto\theta$ via cINNs</li>
            <li>Once trained, amortize inference on arbitrarily many datasets</li>
            <li>Embed data via summary statistics model</li>
          </ul>
          </p>
        </section>


        <section>
          <h2>Generative models</h2>
          generate new data instances, $x\sim\pi(X|Y=y)$
          <img src="img_npe/glow_faces.png" class="r-stretch" />
          <div style="text-align: right;"><small>from: Kingma et al, NeurIPS 2019</small></div>
          e.g.: GANs, VAEs, Flows
        </section>


        <section>
          <h2>Normalizing flows</h2>
          generative models based on an invertible transformation
          <img src="img_npe/normalizing_flows.png" class="r-stretch" />
          <div style="text-align: left;">
            Let $z\sim\mathcal{N}(0, I)$ and $f: z\mapsto x$ bijective.
            Then via change of variable,s the pdf of $x=f(z)$ is given as
            $$p_x(x) = p_z(f^{-1}(x))\cdot|\text{det}(\tfrac{df^{-1}}{dx}(x))|.$$
            <ul>
              <li>in training, transform data points to simple distribution</li>
              <li>trained via negative log-likelihood</li>
              <li>afterwards, generate samples via $f^{-1}(z)$ with $z\sim\mathcal{N}(0,I)$
          </div>
        </section>


        <section>
          <h2>The problem</h2>
          <ul>
            <li>forward model $x_i\sim p(x|\theta) \Leftrightarrow x_i = g(\theta,\xi_i)$ with $\xi_i\sim p(\xi)$</li>
            <li>Bayesian inference $p(\theta|x_{1:N}) \propto p(x_{1:N}|\theta)p(\theta)$</li>
            <li>aim: train an invertible neural network that approximates the true posterior $p_\phi(\theta|x) \approx
              p(\theta|x)$ $\forall \theta,x$</li>
          </ul>
        </section>


        <section>
          <h2>The method</h2>
          Goal: Approximate the true posterior $p_\phi(\theta|x) \approx p(\theta|x)$ $\forall \theta,x$.
          <br /><br />
          Parameterize $p_\phi$ in terms of a cINN given via a bijective $f_\phi:\mathbb{R}^D\rightarrow\mathbb{R}^D$,
          $\theta\mapsto z$, which implements a normalizing flow between $\theta$ and a Gaussian latent variable $z$,
          $$\theta\sim p_\phi(\theta|x) \Leftrightarrow \phi = f^{-1}_\phi(z; x)\quad\text{with}\quad
          z\sim\mathcal{N}_D(z|0,I).$$
          <br /><br />
          Seek neural network parameters $\hat\phi$ that minimize the KL divergence between true and approximate
          posterior $\forall x$, giving the objective ...
        </section>


        <section>
          <h2>The method</h2>
          \begin{equation*}
          \begin{split}
          \hat\phi &= \arg\min_\phi\mathbb{E}_{p(x)}[\text{KL}(p(\theta|x)\!\mid\mid\! p_\phi(\theta|x))]\\
          &= \arg\max_\phi\iint p(x,\theta)\log p_\phi(\theta|x)dxd\theta\\
          &= \arg\max_\phi\iint p(x,\theta)(\log p(f_\phi(\theta;x)) + \log |\det J_{f_\phi}|)dxd\theta
          \end{split}
          \end{equation*}
          Approximate via Monte-Carlo sample:
          \begin{equation*}
          \begin{split}
          \hat\phi &= \arg\min_\phi\frac 1 M\sum_{m=1}^M(-\log p(f_\phi(\theta^{(m)};x^{(m)})) - \log |\det
          J_{f_\phi}^{(m)}|\\
          &= \arg\min_\phi\frac 1 M\sum_{m=1}^M\left(\frac{|f_\phi(\theta^{(m)};x^{(m)})|_2^2}{2} - \log |\det
          J_{f_\phi}^{(m)}|\right)
          \end{split}
          \end{equation*}
        </section>


        <section>
          <h2>Summary statistics learning</h2>
          If data $x_{1:N}$ are high-dimensional: Jointly learn a summary network $\tilde x = h_\psi(x_{1:N})$, giving
          the objective
          $$
          \hat\phi,\hat\psi = \arg\max_{\phi,\psi)}\mathbb{E}_{p(x,\theta,N)}[\log p_\phi(\theta|h_\psi(x_{1:N})]
          $$
          with Monte-Carlo estimate
          $$
          \hat\phi,\hat\psi = \arg\min_{\phi,\psi}\frac 1 M
          \sum_{m=1}^M\left(\frac{|f_\phi(\theta^{(m)};h_\psi(x_{1:N}^{(m)})|_2^2}{2} -
          \log|\det(J_{f_\phi}^{(m)})|\right)
          $$
        </section>


        <section>
          <div style="text-align: left">
            Learning phase:<br />
            <ul>
              <li>create plenty of synthetic data $(y_i,\theta_i)\sim\pi(y,\theta)$</li>
              <li>train a cINN in forward mode</li>
            </ul>
            <br /><br />
            Inference phase:<br />
            <ul>
              <li>sample many latent $z_i\sim\pi(z)$</li>
              <li>run cINN backwards, $\theta_i = g(z_i; y_\text{obs}) \sim \pi(\theta|y_\text{obs})$</li>
            </ul>
            <br /><br />
            <ul style="list-style-type: 'âœ“ ';">
              <li>fast + accurate amortized Bayesian inference</li>
            </ul>
          </div>
        </section>

    </div>
  </div>


  <script src="reveal.js/dist/reveal.js"></script>
  <script src="reveal.js/plugin/zoom/zoom.js"></script>
  <script src="reveal.js/plugin/notes/notes.js"></script>
  <script src="reveal.js/plugin/markdown/markdown.js"></script>
  <script src="reveal.js/plugin/highlight/highlight.js"></script>
  <script src="reveal.js/plugin/math/math.js"></script>
  <script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
      controls: true,
      progress: true,
      center: true,
      hash: true,

      // Learn about plugins: https://revealjs.com/plugins/
      plugins: [RevealMarkdown, RevealHighlight, RevealNotes,
        RevealMath, RevealZoom]
    });
    Reveal.configure({ slideNumber: 'c/t' });
    Reveal.configure({ showSlideNumber: 'none' });

  </script>
</body>

</html>
